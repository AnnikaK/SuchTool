\chapter{Das Vektorraummodell}
\label{vector}
Dieses Kapitel stellt mit dem Vektorraummodell (engl. \textit{vector space model}) ein weiteres klassisches Information-Retrieval-Verfahren vor. \\

\section{Funktionsprinzip}
Wie beim booleschen Retrieval besteht der erste Schritt in der Indexierung der Dokumente und Terme, da die Ermittlung des Vokabulars Ausgangsgrundlage für das weitere Vorgehen ist. \\
Wie der Name bereits nahelegt, basiert das Funktionsprinzip auf Vektoren.
Die Grundidee besteht darin, sowohl für die Suchanfrage als auch für jedes Dokument einen aus reellen Zahlen bestehenden Vektor zu erstellen und anschließend zu ermitteln, zu welchem Dokumentvektor bzw. zu welchen Dokumentvektoren der Anfragevektor die größte Ähnlichkeit besitzt.
Die Länge eines Vektors entspricht hierbei der Anzahl Terme im Vokabular, da im Vektor zu jedem Term dessen Gewicht eingetragen wird. Die Berechnung und Funktion des Gewichts werden in Abschnitt \ref{weights} erklärt. \\
Im Gegensatz zum booleschen Retrieval können die Resultate des Vektorraummodells basierend auf den ermittelten Ähnlichkeitswerten in eine Rangfolge gebracht werden, d.h. mit diesem Verfahren ist Ranking möglich (\cite{Ferber:03}, S.62-63). \\


\section{Vektor und Vektorraum}
Da das Funktionsprinzip des Modells auf Vektoren basiert, werden in diesem Abschnitt die zum Verständnis notwendigen Begriffe \glqq Vektorraum\grqq{} und \grqq Vektor\grqq{} erklärt. 
Vektoren stellen Elemente eines Vektorraumes dar, darum ist es notwendig Letzteres zuerst zu definieren (\cite{Jaenich:84}, S.17). Da hierbei die Vektoroperationen Addition und skalare Multiplikation als bekannt vorausgesetzt werden, seien diese zuvor kurz vorgestellt. Die Definitionen \ref{add} und \ref{mult} stammen beide aus \cite{Jaenich:84}, S.18.
\newpage

\begin{definition} (Addition) \\
	\label{add}
	Sind $(x_1,...,x_n)$ und $(y_1,...,y_n)$ n-Tupel reeller Zahlen, so werde deren Summe durch \\
	$(x_1,...,x_n) + (y_1,...,y_n) = (x_1+y_1,...,x_n+y_n)$\\
	erklärt.
\end{definition}


\begin{definition} (Skalare Multiplikation) \\
	\label{mult}
	Ist $\lambda \in \mathbb{R}$ und $(x_1,...,x_n) \in \mathbb{R}^{n}$, so erklären wir $\lambda (x_1,...,x_n) = (\lambda x_1,...,\lambda x_n) \in \mathbb{R}^{n}$.
\end{definition} 



Ein Vektorraum kann mithilfe der soeben gezeigten Vektoroperationen wie in Definition \ref{vectorraum} (\cite{Jaenich:84}, S.22) angegeben definiert werden. 


\begin{definition}(Vektorraum)  \\
	\label{vectorraum}
	Ein Tripel $(V,+,\cdot)$, bestehend aus einer Menge $V$, einer Abbildung (genannt Addition) \\	
		 $	+ : V \times V \rightarrow V$ \\
    $(x,y) \rightarrow x + y$\\
		    und einer Abbildung (genannt skalare Multiplikation) \\
		    $\cdot : \mathbb{R} \times V \rightarrow  V$ \\
		    $(\lambda,x) \rightarrow \lambda x$ \\
		    heißt \emph{reeller Vektorraum}, wenn für die Abbildungen $+$ und $\cdot$ die folgenden acht Axiome gelten: \\
		    \begin{enumerate}
		    \item  $ (x + y) + z = x + (y + z) $\hspace{13pt}$\forall x,y,z \in V$ \\
			\item  $x + y = y + x$ \hspace{70pt}$\forall x,y \in V$ \\
		    \item Es gibt ein Element $0 \in V$ (genannt \glqq Null{} \grqq oder\glqq  Nullvektor \grqq ) mit\\
			 $x + 0 = x \hspace{97pt} \forall x \in V$ \\
		    \item Zu jedem $x \in V$ gibt es  ein Element $-x \in V$ mit $x + (-x) = 0 $ \\
		    \item $\lambda (\mu x) = (\lambda \mu) x \hspace{78pt}\forall \lambda, \mu \in \mathbb{R}, x\in V$ \\
		    \item  $ 1x = x \hspace{112pt} \forall x \in V$  \\
		    \item $  \lambda (x + y) =  \lambda x + \lambda y \hspace{48pt}\forall \lambda \in \mathbb{R}, x,y \in V$ \\
		    \item $  (\lambda + \mu)x =  \lambda x + \mu x \hspace{48pt}\forall \lambda, \mu \in \mathbb{R}, x \in V$ \\
		    \end{enumerate}
		    

\end{definition}





Nachdem Vektorräume bekannt sind, kann an dieser Stelle auf Vektoren eingegangen werden:
Ein Vektor $\overrightarrow{v} \in V$ ist ein Element des Vektorraums $(V,+,\cdot)$, wenn Addition und skalare Multiplikation die Axiome 1. - 8. aus Definition \ref{vectorraum} erfüllen. 
Zwei Vektoren, die unterschiedlich viele Elemente enthalten, z.B. $\overrightarrow{v_1} = (1,2)$ und $\overrightarrow{v_2} = (1,2,3)$ liegen nicht im selben Vektorraum, weil sie sich weder addieren noch multiplizieren lassen und darum die Axiome nicht erfüllen. Alle Anfrage- und Dokumentvektoren innerhalb eines mit Vektorraummodell realisierten Information-Retrieval-Systems liegen hingegen im selben Vektorraum, da sie auf demselben Vokabular aufbauen und damit die gleiche Länge besitzen.



\section{Definition Vektorraummodell}
Das bereits grob vorgestellte, auf Ähnlichkeiten zwischen Vektoren basierende Funktionsprinzip lässt sich mathematisch mithilfe von Attributen beschreiben. 
Attribute stellen im Vektorraummodell eine Abbildung der Dokumentenmenge $D$ auf die reellen Zahlen $\mathbb{R}$ dar, weshalb der Wertebereich der Attribute im Gegensatz zum booleschen Retrieval auf die reellen Zahlen beschränkt ist.
Das Vektorraummodell lässt sich mittels Attributen wie folgt definieren (\cite{Ferber:03}, S.63):


\begin{definition}\textit{(Vektorraummodell mit Attributen)} 
	
Sei $D = \{d_1,...,d_m\}$ eine Menge von Dokumenten oder Objekten und $A = \{A_1,...,A_n\}$ eine Menge von Attributen $A_j: D \rightarrow \mathbb{R}$ auf diesen Objekten. Die Attributwerte $A_j(d_i) =: w_i,_j$ des Dokuments $d_i$ lassen sich als Gewichte auffassen und zu einem Vektor $w_i = (w_i,_1,...,w_i,_n) \in \mathbb{R}^{n}$ zusammenfassen. Dieser Vektor beschreibt das Dokument im Vektorraummodell: Er ist seine Repräsentation und wird Dokumentvektor genannt. \\
Eine Anfrage wird durch einen Vektor $q \in \mathbb{R}^{n}$ mit Attributwerten, den Anfragevektor oder Query-Vektor, dargestellt. \\
Eine Ähnlichkeitsfunktion $s:\mathbb{R}^{n} \times \mathbb{R}^{n} \rightarrow \mathbb{R}$ definiere für je zwei Vektoren $x,y \in \mathbb{R}^{n}$ einen reellen Ähnlichkeitswert $s(x,y)$.
\end{definition} 
Diese Definition ist aufgrund der Beschreibung durch Attribute sehr allgemein gehalten. Es ist nicht definiert, welche Einheiten des Dokuments gewichtet werden, weshalb theoretisch auch andere Dokumentformate wie etwa Bilder mit Pixeln bzw. Pixelgruppen als Attributen möglich sind (\cite{Ferber:03}, S.63). \\
Im Rahmen dieser Arbeit machen andere Formate als Texte jedoch keinen Sinn, weshalb im Folgenden davon ausgegangen wird, dass ausschließlich Terme gewichtet werden. Spezifisch auf die gegebene Problemstellung bezogen lässt sich die Attributmenge $A$ darum als Termmenge oder Vokabular $T$ auffassen.

\section{Gewichte}
\label{weights}
In diesem Abschnitt wird erklärt, wie und zu welchem Zweck Terme gewichtet werden. 
Bei Gewichten handelt es sich, wie bereits beschrieben, um reelle Zahlenwerte, welche
die Wichtigkeit eines Terms basierend auf dessen statistischer Häufigkeit angeben (\cite{Manning:08}, S.100).

\subsection{Termhäufigkeit}
Die Häufigkeit, mit der ein Term $t$ in einem Dokument $d$ auftaucht, wird als Termhäufigkeit $tf_t,_d$ (engl. \textit{term frequency}) bezeichnet (\cite{Manning:08}, S.71). \\
 Es erscheint intuitiv als logisch, dass ein Dokument, welches das gesuchte Wort mehrmals beinhaltet, wichtiger sein muss als ein Dokument, in dem der Begriff nur ein einziges Mal auftaucht. 
Diese Gewichtungsmethode erlaubt eine viel genauere Differenzierung als eine simple Unterscheidung zwischen \textit{true} und \textit{false}, d.h. zwischen Vorkommen und Fehlen eines Terms, wie es beim booleschen Retrieval der Fall ist.\\
 
\subsection{Dokumenthäufigkeit}
Die Termhäufigkeit alleine stellt keine gute Gewichtungsmethode dar: Eine Bewertung, die ausschließlich von der Termhäufigkeit ausgeht, lässt außer Acht, dass nicht alle Terme gleich wichtig sind. 
Taucht ein Term beispielsweise in jedem Dokument auf, kann er nicht besonders aussagekräftig sein. Deshalb ist es sinnvoll, zusätzlich zur Termhäufigkeit auch die Dokumenthäufigkeit $df_t$ (engl. \textit{document frequency}) zu bestimmen. 
Diese entspricht der Anzahl von Dokumenten in $D$, welche $t$ enthalten. Um den Einfluss nicht aussagekräftiger Terme zu reduzieren, wird das Gewicht umso stärker verringert, je größer die Dokumenthäufigkeit ausfällt (\cite{Manning:08}, S.108). \\

\subsection{Invertierte Dokumenthäufigkeit}
\label{idf}
Um das Gewicht entsprechend der Dokumenthäufigkeit zu verringern, wird als reduzierender Faktor die sogenannte invertierte oder inverse Dokumenthäufigkeit (engl. \textit{inverse document frequency}, kurz IDF) verwendet. Die invertierte Dokumenthäufigkeit $idf_t$ des Terms $t$ wird  wie in Formel \ref{idfa} gezeigt berechnet (\cite{Ferber:03}, S.68).\\


\begin{equation}
\label{idfa}
idf_t =  \frac{1}{df_t}	
\end{equation}   \\

Oftmals werden jedoch modifizierte Formen verwendet, um große Werte seltener Terme durch den Logarithmus wieder zu dämpfen (\cite{Ferber:03}, S.68-69). 
Formel \ref{idfb}  zeigt ein  Beispiel für eine solche modifizierte invertierte Dokumenthäufigkeit, wobei $N$ die Anzahl Dokumente in der Sammlung bezeichnet (\cite{Manning:08}, S.108). 
In der Regel beträgt die Basis des Logarithmus 10, dies spielt aber letztendlich für das korrekte Ranking der Resultate keine Rolle (\cite{Manning:08}, S.109). 
\begin{equation}
\label{idfb}
idf_t = log  \frac{N}{df_t}	
\end{equation}   \\




\subsection{TF-IDF-Gewichtung}
\label{tfidf}
Die vollständige Gewichtungsmethode besteht darin, die Termhäufigkeit mit der invertierten Dokumenthäufigkeit zu multiplizieren. Alle Formeln dieses Typs werden als TF-IDF Gewichtung (engl. \textit{tf-idf weighting}) bezeichnet (\cite{Ferber:03}, S.71, \cite{Manning:08}, S.109). 
Die Berechnung der TF-IDF-Gewichtung für Term $t$ in Dokument $d$ wird in Formel \ref{tfidfa} gezeigt (\cite{Manning:08}, S.109). \\


\begin{equation}
\label{tfidfa}
	tf-idf_t,_d = tf_t,_d \times idf_t
\end{equation}
Verwendet man für die invertierte Dokumenthäufigkeit den unmodifizierten Wert aus Formel \ref{idfa}, so ergibt sich hieraus die Berechnung \ref{tfidfb}.
\begin{equation}
\label{tfidfb}
tf-idf_t,_d = \frac{tf_t,_d}{df_t}
\end{equation}

Für die modifizierte Formel \ref{idfb} lautet die TF-IDF-Gewichtung wie in Formel \ref{tfidfc} angegeben.

\begin{equation}
\label{tfidfc}
tf-idf_t,_d = tf_t,_d \times (log\frac{N} {df_t})
\end{equation}

Sei $T$ das Vokabular, dann enthält der Gewichtsvektor $w_i$ bei Verwendung der TF-IDF-Gewichtung  zu einem Dokument $d_i \in D$ für jeden Term $t_j \in T$ dessen Gewicht $w_i,_j$ = $tf-idf_j,_i$, sodass $w_i = (w_i,_1,...,w_i,_n) = (tf-idf_1,_i,...,tf-idf_n,_i)$ mit $0 < j <= n$ und $n = \#T$ gilt. Mit $\#$ wird die Kardinalität bezeichnet, d.h. die Anzahl der in einer Menge enthaltenen Elemente.

\section{Anfragen}
Beim Vektorraummodell gibt es keine booleschen Operatoren zur Verknüpfung, weshalb Anfragen in Freitextform gestellt werden, d.h. diese können ein oder mehrere Wörter, aber auch ganze Sätze beinhalten. Diese Form wird auch in der Websuche verwendet und ist darum sehr bekannt. 
Da die Reihenfolge von Wörtern weder bei Anfragen noch in den Dokumenten eine Rolle spielt, lassen sich Anfragen einfach als eine Menge von Wörtern bzw. als die daraus resultierende Menge von Termen betrachten.
Ein solches Modell, das lediglich die Anzahl, nicht aber die Reihenfolge von Wörtern berücksichtigt, wird auch als \textit{bag of words model} bezeichnet. \\
Da für jeden in der Anfrage enthaltenen Term ein anderer Ähnlichkeitswert erzielt wird, werden die Werte addiert, sodass pro Dokument ein Gesamtwert berechnet wird (\cite{Manning:08}, S.107). \\
Suchanfragen werden genau wie Dokumente behandelt und die Vektoren wie in Abschnitt \ref{tfidf} beschrieben erstellt(\cite{Ferber:03}, S.82).

%ANPASSEN IN LISP!!!
%-> query tf nicht durch max tf teilen
%-> posting lists für keywords sortieren
%->stop words raushauen
%-> unbedingt nochmal nachpruefen, ob cos-sim so stimmt


\section{Ähnlichkeitsfunktion}
Grundidee des Vektorraummodells ist das Ermitteln der Ähnlichkeit zwischen Vektoren, weshalb hierfür eine geeignete Ähnlichkeitsfunktion benötigt wird. Es bietet sich an, hierzu bekannte Distanzfunktionen für Vektoren zu verwenden, denn eine geringe Distanz impliziert eine hohe Ähnlichkeit.

\subsection{Euklidischer Abstand}
Eine typische Distanzfunktion für Vektoren ist der euklidische Abstand, welcher in Formel \ref{euklid} gezeigt ist (\cite{Manning:08}, S.121).


\begin{equation}
\label{euklid}
|\overrightarrow{x} - \overrightarrow{y}| = \sqrt{\sum_{i = 1}^{M} (x_i - y_i)^{2}}
\end{equation}

Allerdings besitzt der euklidische Abstand den gravierenden Nachteil, dass die Länge der Vektoren für das Ergebnis eine Rolle spielt. 
Reiht man beispielsweise den Inhalt eines Dokumentes $d1$ zweimal aneinander, so besitzt das entstandene Dokument $d2$ für jeden Term die doppelte Termhäufigkeit, sodass der zugehörige Dokumentvektor $\overrightarrow{d2}$ länger ist als $\overrightarrow{d1}$. Bei einer passenden Suchanfrage wird $d2$ einen deutlich höheren Ähnlichkeitswert erzielen als $d1$, obwohl sich die Dokumente inhaltlich nicht unterscheiden.
Das Problem, dass zwei unterschiedlich lange Dokumente, bei denen die darin enthaltenen Terme etwa gleich verteilt sind, dennoch vollkommen verschiedene Ähnlichkeitswerte erzielen, macht den euklidischen Abstand zu einer ungeeigneten Ähnlichkeitsfunktion für das Vektorraummodell.



\subsection{Cosinus-Maß}
Um den Einfluss der Vektorlänge zu eliminieren, wird in der Regel stattdessen das Cosinus-Maß (engl. \textit{cosine similarity}) verwendet, welches den Cosinus des zwischen den Vektoren eingeschlossenen Winkels berechnet. Damit ist dieses Maß von der Vektorlänge unabhängig. \\
Es entspricht dem Skalarprodukt der normalisierten Vektoren (\cite{Manning:08}, S.112). Ein normalisierter Vektor ist ein Vektor, der durch seine euklidische Länge dividiert wird und darum immer die Länge 1 besitzt.
Zur Berechnung des Cosinus-Maßes werden sowohl das Skalarprodukt als auch die euklidische Länge als bekannt vorausgesetzt, darum werden beide an dieser Stelle erklärt.

\subsubsection{Euklidische Länge}
Sei $\overrightarrow{x}$ ein Vektor, dann wird seine euklidische Länge wie in Formel \ref{length} angegeben berechnet (\cite{Manning:08}, S.111).

\begin{equation}
\label{length}
\vert \overrightarrow{x} \vert =\sqrt{\sum_{i = 1}^{M}x_i^{2}}
\end{equation}



\subsubsection{Skalarprodukt}
Das Skalarprodukt zweier Vektoren $\overrightarrow{x}$ und $\overrightarrow{y}$ wird wie in Formel \ref{dot} gezeigt berechnet (\cite{Manning:08}, S.111).

\begin{equation}
\label{dot}
\overrightarrow{x}\cdot \overrightarrow{y}=\sum_{i=1}^{M}x_iy_i
\end{equation}


\subsubsection{Berechnung}
Da das Cosinus-Maß wie bereits erwähnt das Skalarprodukt  der normalisierten Vektoren ist, lautet die Formel wie in \ref{cos} angegeben (\cite{Manning:08}, S.111). Hierbei bezieht sich \textit{sim} auf das englische Wort \textit{similarity}, was übersetzt Ähnlichkeit bedeutet.

\begin{equation}
\label{cos}
sim(\overrightarrow{x},\overrightarrow{y}) = \frac{\overrightarrow{x} \cdot \overrightarrow{y}}{\vert \overrightarrow{x}\vert \cdot \vert \overrightarrow{y} \vert}
\end{equation} 


\begin{figure} [http]
	
	\centering
	\includegraphics[width=0.5\textwidth]{images/vector_space.png}
	\caption{Vektorraum mit den Termen $T1$ und $T2$ als Achsen, drei Dokumentvektoren zu den Dokumenten $d_i$ und einem Anfragevektor zur Anfrage $q$. Das Cosinus-Maß liefert als ähnlichstes Dokument $d_2$, da $\alpha$ der kleinste eingeschlossene Winkel ist (eigene Abbildung, basierend auf \cite{Buettcher:10}, S.55).}
	\label{fancy}

\end{figure}

Um das Cosinus-Maß besser nachvollziehen zu können, hilft eine grafische Veranschaulichung. Abbildung \ref{fancy} zeigt einen Vektorraum mit zwei Termen als Achsen,  sodass der Raum sich zweidimensional darstellen lässt. Bei mehr als zwei Termen ist dies schon nicht mehr möglich, da jeder Term eine eigene Achse im Vektorraum darstellt.
In der Realität gibt es meist weitaus mehr Achsen, da das Vokabular tausende Terme beinhalten kann. \\
Der gezeigte Vektorraum beinhaltet insgesamt drei Dokumentvektoren $\overrightarrow{d_i}$ sowie den Anfragevektor $\overrightarrow{q}$ als Elemente, wobei alle abgebildeten Vektoren bereits normalisiert sind.
 Unter Verwendung des Cosinus-Maßes ergibt sich für $sim(\overrightarrow{d_2}, \overrightarrow{q}) = cos(\alpha)$ der höchsten Wert, da $\alpha$ der kleinste aller Winkel zwischen einem Dokumentvektor $\overrightarrow{d_i}$ und $\overrightarrow{q}$ ist. Hier wird deutlich, dass ausschließlich der Winkel, nicht aber die Länge ausschlaggebend ist.
  Damit wird $d2$ als Ergebnis mit dem höchsten Ähnlichkeitswert ausgegeben (\cite{Buettcher:10}, S.55-56).


%\subsection{Alternativen}





