\chapter{Das Vektorraummodell}
\label{vector}
Dieses Kapitel stellt ein weiteres klassisches Information-Retrieval-Verfahren, das Vektorraummodell, vor. \\

\section{Funktionsprinzip}
Zunächst werden alle Dokumente in Terme zerlegt und indexiert, wie es auch beim booleschen Retrieval der Fall ist. Die Ermittlung des Vokabulars ist Grundvoraussetzung für alle weiteren Schritte. \\
Wie der Name bereits nahelegt, basiert das Verfahren auf Vektoren.

Die Grundidee besteht darin, für jedes Dokument sowie für die Anfrage einen reellen Vektor zu erstellen und anschließend zu ermitteln, welche Dokumentvektoren am ähnlichsten zum Anfragevektor sind. \\
Jeder Vektor besitzt hierbei die Länge des Vokabulars, da er die Gewichte aller Terme enthält. Die Bedeutung des Gewichts wird in Abschnitt \ref{weights} erklärt.\\
Im Gegensatz zum booleschen Retrieval können die Resultate des Vektorraummodells basierend auf dem Grad der Ähnlichkeit in eine Rangfolge gebracht werden (\cite{Ferber:03}, S.62-63). \\


\section{Vektor und Vektorraum}
Da das Funktionsprinzip des Modells auf Vektoren basiert, seien an dieser Stelle die zum Verständnis notwendigen Begriffe Vektorraum und Vektor erklärt. \\
Vektoren stellen Elemente eines Vektorraumes dar, darum ist es notwendig letzteres zuerst zu definieren (\cite{Jaenich:84}, S.17).\\
Ein solcher Vektorraum lässt sich wie in Definition \ref{vectorraum} (\cite{Jaenich:84}, S.22) angegeben beschreiben.
\newpage

\begin{definition}(Vektorraum)  \\
	\label{vectorraum}
	Ein Tripel $(V,+,\cdot)$, bestehend aus einer Menge $V$, einer Abbildung (genannt Addition) \\	
		 $	+ : V \times V \rightarrow V$ \\
    $(x,y) \rightarrow x + y$\\
		    und einer Abbildung (genannt skalare Multiplikation) \\
		    $\cdot : \mathbb{R} \times V \rightarrow  V$ \\
		    $(\lambda,x) \rightarrow \lambda x$ \\
		    heißt \emph{reeller Vektorraum}, wenn für die Abbildungen $+$ und $\cdot$ die folgenden acht Axiome gelten: \\
		    \begin{enumerate}
		    \item  $ (x + y) + z = x + (y + z) $\hspace{13pt}$\forall x,y,z \in V$ \\
			\item  $x + y = y + x$ \hspace{70pt}$\forall x,y \in V$ \\
		    \item Es gibt ein Element $0 \in V$ (genannt \glqq Null{} \grqq oder\glqq  Nullvektor \grqq ) mit\\
			 $x + 0 = x \hspace{97pt} \forall x \in V$ \\
		    \item Zu jedem $x \in V$ gibt es  ein Element $-x \in V$ mit $x + (-x) = 0 $ \\
		    \item $\lambda (\mu x) = (\lambda \mu) x \hspace{78pt}\forall \lambda, \mu \in \mathbb{R}, x\in V$ \\
		    \item  $ 1x = x \hspace{112pt} \forall x \in V$  \\
		    \item $  \lambda (x + y) =  \lambda x + \lambda y \hspace{48pt}\forall \lambda \in \mathbb{R}, x,y \in V$ \\
		    \item $  (\lambda + \mu)x =  \lambda x + \mu x \hspace{48pt}\forall \lambda, \mu \in \mathbb{R}, x \in V$ \\
		    \end{enumerate}
		    

\end{definition}

Hieraus ergibt sich die Frage, wie Addition und skalare Multiplikation bei Vektoren definiert sind, was in Definition \ref{add} und \ref{mult} (\cite{Jaenich:84}, S.18) gezeigt wird.

\begin{definition} (Addition) \\
	\label{add}
	Sind $(x_1,...,x_n)$ und $(y_1,...,y_n)$ n-Tupel reeller Zahlen, so werde deren Summe durch \\
	 $(x_1,...,x_n) + (y_1,...,y_n) = (x_1+y_1,...,x_n+y_n)$\\
	 erklärt.
\end{definition}. 


\begin{definition} (Skalare Multiplikation) \\
	\label{mult}
	Ist $\lambda \in \mathbb{R}$ und $(x_1,...,x_n) \in \mathbb{R}^{n}$, so erklären wir $\lambda (x_1,...,x_n) = (\lambda x_1,...,\lambda x_n) \in \mathbb{R}^{n}$
\end{definition}. 



Ein Vektor $\overrightarrow{v} \in V$ ist demnach ein Element des Vektorraums $(V,+,\cdot)$, wenn  Addition und skalare Multiplikation die Axiome (1) - (8) aus Definition \ref{vectorraum} erfüllen. \\ 
Zwei Vektoren mit unterschiedlich vielen Elementen, z.B. $\overrightarrow{v_1}=(1,2)$ und $\overrightarrow{v_2}=(1,2,3)$ liegen deshalb nicht im selben Vektorraum, weil sie sich weder addieren noch multiplizieren lassen. \\




\section{Definition Vektorraummodell}
Das soeben beschriebene Funktionsprinzip lässt sich mathematisch mithilfe von Attributen beschreiben. 
Attribute stellen im Vektorraummodell eine Abbildung der Dokumentenmenge $D$ auf die reellen Zahlen $\mathbb{R}$ dar, weshalb der Wertebereich der Attribute im Gegensatz zum booleschen Retrieval eindeutig festgelegt ist.
Das Vektorraum lässt sich mittels Attributen wie folgt definieren (\cite{Ferber:03}, S.63):


\begin{definition}\textit{(Vektorraummodell mit Attributen)} \\
	
Sei $D = {d_1,...,d_m}$ eine Menge von Dokumenten oder Objekten und $A = {A_1,...,A_n}$ eine Menge von Attributen $A_j: D \rightarrow \mathbb{R}$ auf diesen Objekten. Die Attributwerte $A_j(d_i) =:w_i,_j$ des Dokuments $d_i$ lassen sich als Gewichte auffassen und zu einem Vektor $w_i = (w_i,_1,...,w_i,_n) \in \mathbb{R}^{n}$ zusammenfassen. Dieser Vektor beschreibt das Dokument im Vektorraummodell: Er ist seine Repräsentation und wird Dokumentvektor genannt. \\
Eine Anfrage wird durch einen Vektor $q \in \mathbb{R}^{n}$ mit Attributwerten, den Anfragevektor oder Query-Vektor, dargestellt. \\
Eine Ähnlichkeitsfunktion $s:\mathbb{R}^{n} \times \mathbb{R}^{n} \rightarrow \mathbb{R}$ definiere für je zwei Vektoren $x,y \in \mathbb{R}^{n}$ einen reellen Ähnlichkeitswert $s(x,y)$.
\end{definition} 
Diese Definition ist aufgrund der Beschreibung durch Attribute sehr allgemein gehalten. Es ist deshalb nicht definiert, welche Einheiten des Dokuments gewichtet werden. Demnach sind theoretisch auch andere Dokumentformate wie etwa Bilder mit Pixeln bzw. Pixelgruppen als Attributen möglich (\cite{Ferber:03}, S.63). \\
Praktisch gesehen machen im  Rahmen dieser Arbeit jedoch andere Formate als Texte keinen Sinn. Darum wird im folgenden davon ausgegangen, dass ausschließlich Terme gewichtet werden. Demnach lässt sich die Attributmenge $A$ spezifisch auf die Problemstellung bezogen als Menge der Terme oder Vokabular $T$ auffassen.

\section{Gewichte}
\label{weights}
Bei Gewichten handelt es sich, wie bereits beschrieben, um reelle Zahlenwerte.\\
Ein Gewicht gibt die Wichtigkeit eines Terms basierend auf dessen statistischer Häufigkeit an (\cite{Manning:08}, S.100).\\

\subsection{Termhäufigkeit}
Die Häufigkeit, mit der ein Term $t$ in einem Dokument $d$ auftritt, wird als Termhäufigkeit (engl. term frequency) bezeichnet. Demnach wird die Termhäufigkeit pro Vorkommen von $t$ in $d$ um eins erhöht (\cite{Manning:08}, S.71). \\
 Es erscheint intuitiv logisch, dass ein Text, der das gesuchte Wort mehrmals beinhaltet wichtiger sein muss als ein Dokument, in welchem der Begriff nur ein einziges mal auftaucht. \\
Dies Gewichtungsschema erlaubt eine viel genauere Differenzierung als eine simple Unterscheidung zwischen true und false, wie es beim booleschen Retrieval der Fall ist.\\
 
\subsection{Dokumenthäufigkeit}
Die Termhäufigkeit stellt für sich genommen schon eine mögliche Gewichtungsmethode dar, allerdings keine besonders gute: Die Bewertung alleine aufgrund der Termhäufigkeit lässt außer Acht, dass nicht alle Terme gleich wichtig sind. \\
Taucht ein Term beispielsweise in jedem Dokument auf, kann es nicht besonders aussagekräftig sein. Demnach ist es sinnvoll, zusätzlich zur Termhäufigkeit $tf_t,_d$ auch die Dokumenthäufigkeit (engl. document frequency) $df_t$  zu bestimmen. \\
Diese entspricht der Anzahl Dokumente in $D$, welche $t$ enthalten. Um den Einfluss nicht aussagekräftiger Terme zu reduzieren, wird das Gewicht umso stärker verringert, je größer die Dokumenthäufigkeit ausfällt (\cite{Manning:08}, S.108). \\

\subsection{Invertierte Dokumenthäufigkeit}
\label{idf}
Zur Reduktion des Gewichts basierend auf der Dokumenthäufigkeit wird als reduzierender Faktor die Invertierte Dokumenthäufigkeit (engl. inverse document frequency) verwendet. Die invertierte Dokumenthäufigkeit $idf_t$ des Terms $t$ berechnet sich wie in Formel \ref{idfa} gezeigt.\\


\begin{equation}
\label{idfa}
idf_t =  \frac{1}{df_t}	
\end{equation}   \\

Oftmals werden modifizierte Formen verwendet, um die großen Werte seltener Terme durch den Logarithmus wieder zu dämpfen (\cite{Ferber:03}, S.68-69). \\
Formel \ref{idfb} zeigt ein  Beispiel für eine solche modifizierte invertierte Dokumenthäufigkeit.
In der Regel beträgt die Basis des Logarithmus 10, dies spielt aber letztendlich für das korrekte Ranking der Resultate keine Rolle  (\cite{Manning:08}, S.108-109). \\ 
\begin{equation}
\label{idfb}
idf_t = log  \frac{N}{df_t}	
\end{equation}   \\




\subsection{TF-IDF-Gewichtung}
\label{tfidf}
Die vollständige Methode zur Gewichtung einzelner Terme kombiniert Termhäufigkeit und invertierte Dokumenthäufigkeit, indem erstere mit letzterer multipliziert wird. Alle Formeln dieses Typs werden als TF-IDF Gewichtung (engl. tf-idf weighting) bezeichnet (\cite{Ferber:03}, S.71). \\ 
Das Gewicht für Term $t$ in Dokument $d$ berechnet sich somit wie in Formel \ref{tfidfa} gezeigt (\cite{Manning:08}, S.109). \\


\begin{equation}
\label{tfidfa}
	tf-idf_t,_d = tf_t,_d \times idf_t
\end{equation}
Verwendet man für die invertierte Dokumenthäufigkeit den unmodifizierten Wert \ref{idfa}, so ergibt sich hieraus die Berechnung:
\begin{equation}
\label{tfidfb}
tf-idf_t,_d = \frac{tf_t,_d}{df_t}
\end{equation}

Für die modifizierte Formel \ref{idfb} lautet die TF-IDF-Gewichtung wie folgt:

\begin{equation}
\label{tfidfc}
tf-idf_t,_d = tf_t,_d \times (log\frac{N} {df_t})
\end{equation}

Sei $T$ die Menge aller Terme der Sammlung bzw. das Vokabular, dann enthält der Gewichtsvektor $w_i$ zu einem Dokument $d_i \in D$ für jeden Term $t_j \in T$ dessen Gewicht $w_i,_j$ = $tf-idf_j,_i$, sodass $w_i=(tf-idf_1,_i,...,tf-idf_n,_i)$ gilt.

\section{Anfragen}
Beim Vektorraummodell gibt es keine booleschen Operatoren zur Verknüpfung von Termen, weshalb Anfragen in Freitextform gestellt werden. Diese Form wird auch in der Websuche verwendet und ist darum sehr bekannt. \\
Da die Reihenfolge von Wörtern weder bei Anfragen noch in den Dokumenten eine Rolle spielt, lassen sich Anfragen einfach als eine Menge von Wörtern bzw. als die daraus resultierende Menge von Termen betrachten.\\
 Ein solches Modell, das lediglich die Anzahl, nicht aber die Reihenfolge von Wörtern berücksichtigt, wird auch als \textit{bag of words model} bezeichnet. \\
Da für jeden Term ein anderer Ähnlichkeitswert erzielt wird, werden die Ähnlichkeitswerte aller in der Menge enthaltenen Terme addiert, sodass pro Dokument ein Gesamtwert berechnet wird (\cite{Manning:08}, S.107). \\
Anfragetexte werden genau wie Dokumente behandelt und die Vektoren wie in Abschnitt \ref{tfidf} beschrieben bestimmt (\cite{Ferber:03}, S.82).

%ANPASSEN IN LISP!!!
%-> query tf nicht durch max tf teilen
%-> posting lists für keywords sortieren
%->stop words raushauen
%-> unbedingt nochmal nachpruefen, ob cos-sim so stimmt


\section{\"Ahnlichkeitsfunktion}
Grundidee des Vektorraummodells ist das Ermitteln der Ähnlichkeit zwischen Vektoren. Deshalb muss hierfür eine geeignete Ähnlichkeitsfunktion gefunden werden.

\subsection{Euklidischer Abstand}
Eine typische Distanzfunktion für Vektoren ist der euklidische Abstand, bei dem die Differenz wie in Formel \ref{euklid} gezeigt berechnet wird (\cite{Bryson:14},S.132). \\


\begin{equation}
\label{euklid}
d(\overrightarrow{x},\overrightarrow{y}) = \sqrt{\sum_{i=1}^{M} (x_i - y_i)^{2}}
\end{equation}

Allerdings besitzt der euklidische Abstand den gravierenden Nachteil, dass die Länge der Vektoren eine Rolle spielt. \\
Liegen zwei Dokumente $d1$ und $d2$ vor und $d2$ besitzt den Inhalt von $d1$ zweimal aneinandergereiht, so wird $d2$ als ähnlicher eingestuft, aus dem einzigen Grund weil die Termhäufigkeit doppelt so groß ist und der Vektor damit die doppelte Länge hat. \\
Das Problem, dass zwei unterschiedlich lange Dokumente, in denen die gesuchten Terme etwa gleich verteilt sind, dennoch vollkommen verschiedene Ähnlichkeitswerte erzielen macht den euklidischen Abstand zu einer ungeeigneten Ähnlichkeitsfunktion.



\subsection{Cosinus-Maß}
Um den Einfluss der Vektorlänge zu eliminieren, wird in der Regel das Cosinus-Maß verwendet. \\
Das Cosinus-Maß ist das Skalarprodukt der normalisierten Vektoren (\cite{Manning:08}, S.112). Ein normalisierter Vektor besitzt immer die Länge 1, da Ursprungsvektor durch die euklidische Länge dividiert wird.\\
Zur Berechnung des Cosinus-Maßes müssen somit sowohl das Skalarprodukt als auch die Längenberechnung eines Vektors bekannt sein, darum werden beide an dieser Stelle erklärt.

\subsubsection{Euklidische Länge}
Sei $\overrightarrow{x}$ ein Vektor, dann wird seine euklidische Länge wie in Formel \ref{length} angegeben berechnet (\cite{Manning:08}, S.111).

\begin{equation}
\label{length}
\vert \overrightarrow{x} \vert =\sqrt{\sum_{i=1}^{M}x_i^{2}}
\end{equation}



\subsubsection{Skalarprodukt}
Das Skalarprodukt zweier Vektoren $\overrightarrow{x}$ und $\overrightarrow{y}$ wird wie in Formel \ref{dot} gezeigt berechnet (\cite{Manning:08}, S.111).

\begin{equation}
\label{dot}
\overrightarrow{x}\cdot \overrightarrow{y}=\sum_{i=1}^{M}x_iy_i
\end{equation}


\subsubsection{Funktion}
Das Cosinus-Maß multipliziert die Vektoren und dividiert sie durch deren Länge, sodass die Ähnlichkeitsfunktion von der Länge unbeeinflusst ist, was in Formel \ref{cos} gezeigt wird (\cite{Manning:08}, S.111).
\begin{equation}
\label{cos}
sim(\overrightarrow{x},\overrightarrow{y}) = \frac{\overrightarrow{x} \cdot \overrightarrow{y}}{\vert \overrightarrow{x}\vert \cdot \vert \overrightarrow{y} \vert}
\end{equation} 


\begin{figure} [http]
	
	\centering
	\includegraphics[width=0.5\textwidth]{images/vector_space.png}
	\caption{Vektorraum mit den Termen $T1$ und $T2$ als Achsen, drei Dokumentvektoren und dem Anfragevektor. Das Cosinus-Maß liefert als ähnlichstes Dokument $d_2$ (Eigene Abbildung, basierend auf \cite{Buettcher:10}, S.55).}
	\label{fancy}

\end{figure}

Zum besseren Verständnis des Cosinus-Maßes zeigt die Beispielabbildung \ref{fancy} einen Vektorraum mit zwei Termen als Achsen, sodass der Raum sich zweidimensional darstellen lässt. In der Realität gibt es meist weitaus mehr Achsen, da das Vokabular tausende Terme beinhalten kann.\\
Der Vektorraum beinhaltet insgesamt drei Dokumentvektoren $\overrightarrow{d_i}$ sowie den Anfragevektor $\overrightarrow{q}$ als Elemente, wobei alle Vektoren normalisiert sind.\\
 Unter Verwendung des Cosinus-Maßes ergibt sich für $sim(\overrightarrow{d_2}, \overrightarrow{q})=cos(\alpha)$ der höchsten Wert, da $\alpha$ der kleinste aller Winkel zwischen einem Dokumentvektor $\overrightarrow{d_i}$ und $\overrightarrow{q}$ ist. Hier wird deutlich, dass lediglich der Winkel, nicht aber die Länge relevant ist.\\
  Damit wird $d2$ als erstes Ergebnisdokument ausgegeben (\cite{Buettcher:10}, S.55-56).


%\subsection{Alternativen}





