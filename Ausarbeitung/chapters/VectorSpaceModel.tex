\chapter{Das Vektorraummodell}
\label{vector}
Dieses Kapitel stellt ein weiteres klassisches Information Retrieval Verfahren, das Vektorraummodell, vor. \\

\section{Funktionsprinzip}
Zunächst werden alle Dokumente in Terme zerlegt und indexiert, wie es auch beim booleschen Retrieval der Fall ist. Die Ermittlung des Vokabulars ist Grundvoraussetzung für alle weiteren Schritte. \\
Wie der Name bereits nahelegt, basiert das Verfahren auf Vektoren.
%definition vektor?
Die Grundidee besteht darin, für jedes Dokument sowie für die Anfrage einen reellen Vektor zu erstellen und anschließend zu ermitteln, welche Dokumentvektoren am ähnlichsten zum Anfragevektor sind. \\
Jeder Vektor besitzt hierbei die Länge des Vokabulars, da er die Gewichte aller Terme enthält. Die Bedeutung des Gewichts wird in Abschnitt \ref{weights} erklärt.\\
Im Gegensatz zum booleschen Retrieval können die Resultate des Vektorraummodells basierend auf dem Grad der Ähnlichkeit in eine Rangfolge gebracht werden (\cite{Ferber:03}, S.62-63). \\


\section{Vektor und Vektorraum}
Um die Funktionsweise des Vektorraummodells zu verstehen, müssen zunächst die Begriffe Vektor und Vektorraum bekannt sein.

\subsection{Definition Vektor}
\subsection{Definiton Vektorraum}

\section{Definition Vektorraummodell}
Das soeben beschriebene Funktionsprinzip lässt sich mathematisch mithilfe von Attributen beschreiben. \\
Attribute stellen im Vektorraummodell eine Abbildung der Dokumentenmenge $D$ auf die reellen Zahlen $R$ dar. Damit ist der Wertebereich der Attribute im Gegensatz zum booleschen Retrieval eindeutig festgelegt.\\
Die Definition lautet somit wie folgt: \\


\begin{definition}\textit{(Vektorraummodell mit Attributen)} \\
	
Sei $D = {d_1,...,d_n}$ eine Menge von Dokumenten oder Objekten und $A = {A_1,...,A_n}$ eine Menge von Attributen $A_j: D \rightarrow R$ auf diesen Objekten. Die Attributwerte $A_j(d_i) =:w_i,_j$ des Dokuments $d_i$ lassen sich als Gewichte auffassen und zu einem Vektor $w_i = (w_i,_1,...,w_i,_n) \in R^{n}$ zusammenfassen. Dieser Vektor beschreibt das Dokument im Vektorraummodell: Er ist seine Repräsentation und wird Dokumentvektor genannt. \\
Eine Anfrage wird durch einen Vektor $q \in R^{n}$ mit Attributwerten, den Anfragevektor oder Query-Vektor, dargestellt. \\
Eine Ähnlichkeitsfunktion $s: R^{n} \times R^{n} \rightarrow R$ definiere für je zwei Vektoren $x,y \in R^{n}$ einen reellen Ähnlichkeitswert $s(x,y)$.
\end{definition} 
Diese Definition ist aufgrund der Beschreibung durch Attribute sehr allgemein gehalten. Es ist deshalb nicht definiert, welche Einheiten des Dokuments gewichtet werden. Demnach sind theoretisch auch andere Dokumentformate wie etwa Bilder mit Pixeln bzw. Pixelgruppen als Attributen möglich (\cite{Ferber:03}, S.63). \\
Praktisch gesehen machen im  Rahmen dieser Arbeit jedoch andere Formate als Texte keinen Sinn. Darum wird im folgenden davon ausgegangen, dass ausschließlich Terme gewichtet werden. Demnach lässt sich die Attributmenge $A$ spezifisch auf die Problemstellung bezogen als Menge der Terme oder Vokabular $T$ auffassen.

\section{Gewichte}
\label{weights}
Bei Gewichten handelt es sich, wie bereits beschrieben, um reelle Zahlenwerte.\\
Ein Gewicht gibt die Wichtigkeit eines Terms basierend auf dessen statistischer Häufigkeit an (\cite{Manning:08}, S.100).\\

\subsection{Termhäufigkeit}
Die Häufigkeit, mit der ein Term $t$ in einem Dokument $d$ auftritt, wird als Termhäufigkeit (engl. term frequency) bezeichnet. Demnach wird die Termhäufigkeit pro Vorkommen von $t$ in $d$ um eins erhöht (\cite{Manning:08}, S.71). \\
 Es erscheint intuitiv logisch, dass ein Text, der das gesuchte Wort mehrmals beinhaltet wichtiger sein muss als ein Dokument, in welchem der Begriff nur ein einziges mal auftaucht. \\
Dies Gewichtungsschema erlaubt eine viel genauere Differenzierung als eine simple Unterscheidung zwischen true und false, wie es beim booleschen Retrieval der Fall ist.\\
 
\subsection{Dokumenthäufigkeit}
Die Termhäufigkeit stellt für sich genommen schon eine mögliche Gewichtungsmethode dar, allerdings keine besonders gute: Die Bewertung alleine aufgrund der Termhäufigkeit lässt außer Acht, dass nicht alle Terme gleich wichtig sind. \\
Taucht ein Term beispielsweise in jedem Dokument auf, kann es nicht besonders aussagekräftig sein. Demnach ist es sinnvoll, zusätzlich zur Termhäufigkeit $tf_t,_d$ auch die Dokumenthäufigkeit (engl. document frequency) $df_t$  zu bestimmen. \\
Diese entspricht der Anzahl Dokumente in $D$, welche $t$ enthalten. Um den Einfluss nicht aussagekräftiger Terme zu reduzieren, wird das Gewicht umso stärker verringert, je größer die Dokumenthäufigkeit ausfällt (\cite{Manning:08}, S.108). \\

\subsection{Invertierte Dokumenthäufigkeit}
Zur Reduktion des Gewichts basierend auf der Dokumenthäufigkeit wird als reduzierender Faktor die Invertierte Dokumenthäufigkeit (engl. inverse document frequency) verwendet. Die invertierte Dokumenthäufigkeit $idf_t$ des Terms $t$ berechnet sich wie in Formel \ref{idfa} gezeigt.\\


\begin{equation}
\label{idfa}
idf_t =  \frac{1}{df_t}	
\end{equation}   \\

Oftmals werden modifizierte Formen verwendet, um die großen Werte seltener Terme durch den Logarithmus wieder zu dämpfen (\cite{Ferber:03}, S.68-69). \\
Formel \ref{idfb} zeigt ein  Beispiel für eine solche modifizierte invertierte Dokumenthäufigkeit.
In der Regel beträgt die Basis des Logarithmus 10, dies spielt aber letztendlich für das korrekte Ranking der Resultate keine Rolle  (\cite{Manning:08}, S.108-109). \\ 
\begin{equation}
\label{idfb}
idf_t = log  \frac{N}{df_t}	
\end{equation}   \\




\subsection{TF-IDF-Gewichtung}
\label{tfidf}
Die vollständige Methode zur Gewichtung einzelner Terme kombiniert Termhäufigkeit und invertierte Dokumenthäufigkeit, indem erstere mit letzterer multipliziert wird. Alle Formeln dieses Typs werden als TF-IDF Gewichtung (engl. tf-idf weighting) bezeichnet (\cite{Ferber:03}, S.71). \\ 
Das Gewicht für Term $t$ in Dokument $d$ berechnet sich somit wie in Formel \ref{tfidfa} gezeigt (\cite{Manning:08}, S.109). \\


\begin{equation}
\label{tfidfa}
	tf-idf_t,_d = tf_t,_d \times idf_t,
\end{equation}
Verwendet man für die invertierte Dokumenthäufigkeit den unmodifizierten Wert \ref{idfa}, so ergibt sich hieraus die Berechnung:
\begin{equation}
\label{tfidfb}
tf-idf_t,_d = \frac{tf_t,_d}{ df_t}
\end{equation}

Für die modifizierte Formel \ref{idfb} lautet die TF-IDF-Gewichtung wie folgt:

\begin{equation}
\label{tfidfc}
tf-idf_t,_d = tf_t,_d \times (log\frac{N} {df_t})
\end{equation}

Sei $T$ die Menge aller Terme der Sammlung bzw. das Vokabular, dann enthält der Gewichtsvektor $w_i$ zu einem Dokument $d_i \in D$ für jeden Term $t_j \in T$ dessen Gewicht $w_i,_j$ = $tf-idf_j,_i$, sodass $w_i=(tf-idf_1,_i,...,tf-idf_n,_i)$ gilt.

\section{Anfragen}
Beim Vektorraummodell gibt es keine booleschen Operatoren zur Verknüpfung von Termen, weshalb Anfragen in Freitextform gestellt werden. Diese Form wird auch in der Websuche verwendet und ist darum sehr bekannt. \\
Da die Reihenfolge von Wörtern weder bei Anfragen noch in den Dokumenten eine Rolle spielt, lassen sich Anfragen einfach als eine Menge von Wörtern bzw. als die daraus resultierende Menge von Termen betrachten.\\
 Ein solches Modell, das lediglich die Anzahl, nicht aber die Reihenfolge von Wörtern berücksichtigt, wird auch als \textit{bag of words model} bezeichnet. \\
Da für jeden Term ein anderer Ähnlichkeitswert erzielt wird, werden die Ähnlichkeitswerte aller in der Menge enthaltenen Terme addiert, sodass pro Dokument ein Gesamtwert berechnet wird (\cite{Manning:08}, S.107). \\
Anfragetexte werden genau wie Dokumente behandelt und die Vektoren wie in Abschnitt \ref{tfidf} beschrieben bestimmt (\cite{Ferber:03}, S.82).

%ANPASSEN IN LISP!!!
%-> query tf nicht durch max tf teilen
%-> posting lists für keywords sortieren
%->stop words raushauen
%-> unbedingt nochmal nachpruefen, ob cos-sim so stimmt


\section{\"Ahnlichkeitsfunktion}
Grundidee des Vektorraummodells ist das Ermitteln der Ähnlichkeit zwischen Vektoren. Deshalb muss hierfür eine geeignete Ähnlichkeitsfunktion gefunden werden.

\subsection{Euklidischer Abstand}
Eine typische Distanzfunktion für Vektoren ist der euklidische Abstand, bei dem die Differenz wie in Formel \ref{euklid} gezeigt berechnet wird (\cite{Bryson:14},S.132). \\


\begin{equation}
\label{euklid}
d(\overrightarrow{x},\overrightarrow{y}) = \sqrt{\sum_{i=1}^{M} (x_i - y_i)^{2}}
\end{equation}

Allerdings besitzt der euklidische Abstand den gravierenden Nachteil, dass die Länge der Vektoren eine Rolle spielt. \\
Liegen zwei Dokumente $d1$ und $d2$ vor und $d2$ besitzt den Inhalt von $d1$ zweimal aneinandergereiht, so wird $d2$ als ähnlicher eingestuft, aus dem einzigen Grund weil die Termhäufigkeit doppelt so groß ist und der Vektor damit die doppelte Länge hat. \\
Das Problem, dass zwei unterschiedlich lange Dokumente, in denen die gesuchten Terme etwa gleich verteilt sind, dennoch vollkommen verschiedene Ähnlichkeitswerte erzielen macht den euklidischen Abstand zu einer ungeeigneten Ähnlichkeitsfunktion.



\subsection{Cosinus-Maß}
Um den Einfluss der Vektorlänge zu eliminieren, wird in der Regel das Cosinus-Maß verwendet. 
Das Cosinus-Maß ist das Skalarprodukt der normalisierten Vektoren (\cite{Manning:08}, S.112), d.h. die Vektoren werden durch ihre Länge dividiert. \\
Da zur Berechnung des Cosinus-Maßes somit sowohl das Skalarprodukt als auch die Längenberechnung eines Vektors bekannt sein müssen, werden beide an diese Stelle vorgestellt.

\subsubsection{Euklidische Länge}
Sei $\overrightarrow{x}$ ein Vektor, dann wird seine euklidische Länge wie in Formel \ref{length} angegeben berechnet.

\begin{equation}
\label{length}
\vert \overrightarrow{x} \vert =\sqrt{\sum_{i=1}^{M}x_i^{2}}
\end{equation}



\subsubsection{Skalarprodukt}
Das Skalarprodukt zweier Vektoren $\overrightarrow{x}$ und $\overrightarrow{y}$ wird wie in Formel \ref{dot} gezeigt berechnet.

\begin{equation}
\label{dot}
\overrightarrow{x}\cdot \overrightarrow{y}=\sum_{i=1}^{M}x_iy_i
\end{equation}


\subsubsection{Funktion}
Das Cosinus-Maß multipliziert die Vektoren und dividiert sie durch deren Länge, sodass die Ähnlichkeitsfunktion von der Länge unbeeinflusst ist, was in Formel \ref{cos} gezeigt wird (\cite{Manning:08}, S.111).
\begin{equation}
\label{cos}
sim(\overrightarrow{x},\overrightarrow{y}) = \frac{\overrightarrow{x} \cdot \overrightarrow{y}}{\vert \overrightarrow{x}\vert \cdot \vert \overrightarrow{y} \vert}
\end{equation} 

\subsubsection{Beispiel}
\begin{figure} [http]
	
	\centering
	\includegraphics[width=0.5\textwidth]{images/vector_space.png}
	\caption{Vektorraum mit den Termen $T1$ und $T2$ als Achsen, drei Dokumentvektoren und dem Anfragevektor. Das Cosinus-Maß liefert als ähnlichsten Dokumentvektor $d_2$ (Eigene Abbildung).}
	\label{fancy}

\end{figure}

Abbildung \ref{fancy} zeigt einen Vektorraum mit zwei Termen als Achsen, sodass er sich zweidimensional darstellen lässt. In der Realität gibt es meist weitaus mehr Achsen, da das Vokabular tausende Terme beinhalten kann. Die hier dargestellten Vektoren wurden bereits normalisiert.\\
Dieser beispielhafte Vektorraum beinhaltet insgesamt drei Dokumentvektoren $\overrightarrow{d_i}$ sowie den Anfragevektor $\overrightarrow{q}$.\\
 Unter Verwendung des Cosinus-Maßes ergibt sich für $sim(\overrightarrow{d_2}, \overrightarrow{q})=cos(\alpha)$ der höchsten Wert, da $\alpha$ der kleinste Winkel ist. Damit wird $d2$ als erstes Ergebnisdokument ausgegeben (\cite{Manning:08}, S.112).

%hier noch andere Quelle angeben (Bild)

\subsection{Alternativen}





