\chapter{Bewertung eines Information-Retrieval-Systems}
In diesem Kapitel wird erläutert, wie sich Information-Retrieval-Systeme bewerten und vergleichen lassen. \\
 
Nachdem mit dem booleschen Retrieval und dem Vektorraummodell bereits zwei klassische Verfahren vorgestellt wurden, mit denen sich Information-Retrieval-Systeme realisieren lassen, liegt es nahe, nach einer Methode zum Bewerten und Vergleichen solcher System zu suchen. 
Die Bewertungsmethode muss hierbei geeignet sein, verschiedenste Systeme zu bewerten, denn diese können sich in zahlreichen Punkten wie etwa Dokumentformat, Dokumentrepräsentation, der Art und Weise, wie Anfragen formuliert und Ergebnisse präsentiert werden, unterscheiden (\cite{Ferber:03}, S.84). \\

\section{Problem Relevanz}
Um ein Information-Retrieval-System hinsichtlich dessen Qualität beurteilen zu können, muss die Relevanz der zurückgelieferten Ergebnisse eingestuft werden.
Hiermit eröffnet sich das Hauptproblem: Wann ist ein Dokument relevant? Allgemein formuliert lässt sich dies so beantworten: Ein Dokument ist relevant, wenn es das in der Anfrage formulierte Informationsbedürfnis des Nutzers befriedigt (\cite{Ferber:03}, S.85). 
Der Begriff Relevanz lässt sich zudem mathematisch wie in Definition \ref{Relevance} (\cite{Ferber:03}, S.86) angegeben definieren. \\ 

\begin{definition}(Relevanz)\\
	\label{Relevance}
	Die Relevanz eines Dokuments für eine Anfrage ist eine Relation $r: D \times Q \rightarrow R$, wobei $D={D_1,...,d_m}$ die Menge der Dokumente, $Q$ die Menge der Anfragen und $R$ eine Menge von Wahrheitswerten, im Allgemeinen die Menge ${0,1}$, ist. (Im Folgenden wird $R=\{0,1\}$ angenommen, wenn nichts anderes gesagt wird.)\\
	Die Relation $r$ wird im Allgemeinen durch Befragen von Experten zu konkreten Anfragen und Dokumentmengen ermittelt und als Tabelle oder in Form von Listen gespeichert. \\
\end{definition}

Da zum Bestimmen der Relation die Beurteilung von Experten erforderlich ist, lässt diese Definition sofort erkennen, dass Relevanz stets von der subjektiven Wahrnehmung eines Anwenders abhängig ist: Jeder Nutzer entscheidet für sich selbst, ob er ein Dokument als relevant einstuft.

\section{Precision und Recall}
In diesem Abschnitt werden die beiden zur Bewertung notwendigen Evaluierungsmaße Precision und Recall vorgestellt.

\subsection{Precisison}
Precision bedeutet übersetzt Präzision und bezeichnet den Anteil relevanter Dokumente an allen zurückgelieferten Dokumenten. Formel \ref{prec} beschreibt die Berechnung, wobei $\#$ für die Kardinalität der Menge, d.h. die Anzahl der darin enthaltenen Elemente, steht.

\begin{equation}
\label{prec}
Precision = \frac{\#(relevant \hspace{2mm} items \hspace{2mm} retrieved)}{\#(retrieved \hspace{2mm} items)}
\end{equation}


\subsection{Recall}
Recall kann mit Trefferquote übersetzt werden und beschreibt die Frage, wie viele der relevanten Dokumente tatsächlich vom System zurückgeliefert wurden, was in Formel \ref{rec} gezeigt wird (\cite{Manning:08}, S.142-143). 

\begin{equation}
\label{rec}
Recall = \frac{\#(relevant \hspace{2mm} items \hspace{2mm} retrieved)}{\#(relevant \hspace{2mm} items)}
\end{equation}

\subsection{Veranschaulichung}
Die Bedeutung der Maße lässt sich leichter anhand der Kontingenztafel \ref{tabelle} nachvollziehen. 
Kontingenztafeln sind Häufigkeitstabellen und stammen aus der Statistik. Sie  beschreiben die gemeinsame Verteilung zweier Merkmale, in diesem Fall handelt es sich hierbei um Relavanz und Rückgewinnung (\cite{Engelhardt:14}). 

 \begin{table}
	\caption{Kontingenztafel (\cite{Manning:08}, S.143)}
	\begin{tabular}{l|l|l}
		\space & relevant & irrelevant \\
		\hline
		zurückgeliefert & true positives (tp) & false positives (fp) \\
		\hline
		nicht zurückgeliefert & false negatives (fn) & true negatives (tn)
	\end{tabular}
	\label{tabelle}
\end{table}

Das Precision-Evaluierungsmaß lässt sich anhand Tabelle \ref{tabelle} wie folgt beschreiben:

\begin{equation}
Precision = \frac{tp}{tp+fp}
\end{equation}

Präzision berechnet demnach, wie viele der als positiv eingestuften Ergebnisse, d.h. inklusive der \textit{false positives}, auch tatsächlich \textit{true postives}, d.h. relevante Resultate, sind.\\

Die Beschreibung für das Recall-Evaluierungsmaß anhand Tabelle \ref{tabelle} lautet wie folgt:

\begin{equation}
Recall = \frac{tp}{tp+fn}
\end{equation}

Für die Bestimmung des Recalls werden die \textit{true positives}, d.h. alle vom System korrekt als positiv eingestuften Ergebnisse, durch die Gesamtheit der relevanten Resultate dividiert. Wie sich aus der Tabelle entnehmen lässt, sind dies die \textit{true positives} und die \textit{false negatives}, d.h. auch jene Dokumente, die fälschlicherweise vom System als nicht relevant eingestuft wurden. Der Recall gibt deshalb kurz gesagt die Trefferquote an.



\section{Zwei Evaluierungsmaße}
Um ein System aussagekräftig bewerten zu können, reicht eines der Maße nicht aus. 
Beispielsweise könnte ein System einen Recall von $100\%$ erreichen, indem es einfach alle Dokumente zurückliefert. 
Umgekehrt lässt sich auch eine Precision von $100\%$ erzielen, wenn nur ein einziges Dokument gefunden wurde und dieses ein Treffer war. Vielleicht wurden hier allerdings eine ganze Reihe weiterer relevanter Dokumente nicht gefunden.
Deshalb werden stets beide Maße für eine qualitative Einschätzung eines Information-Retrieval-Systems benötigt.

\section{Durchführung}
Eine Bewertung anhand der oben aufgeführten Evaluierungsmaße kann durchgeführt werden, wenn die folgenden Voraussetzungen gegeben sind (\cite{Manning:08}, S.140): \\

\begin{itemize}
	\item Vorgegebene Dokumentsammlung
	\item Feste Menge von Test-Anfragen
	\item Eine Relation $r$, die jedem Anfragen-Dokument Paar einen Wert $\in \{0,1\}$ für relevant bzw. irrelevant zuordnet.
\end{itemize}
Leider sind für diese Arbeit jedoch die oben gelisteten Voraussetzungen nicht gegeben.
 Die Erzeugung einer repräsentativen Test-Dokumentsammlung ist für die Problemstellung nicht möglich, da die Art des Dokumentarchivs offen gehalten wurde. 
Auch mit Beschränkung auf den Anwendungsfall E-Mails wäre in jedem Fall die zu erzeugende Anfragenmenge zu groß, um sie im Rahmen dieser Arbeit bewältigen zu können, da Anfragen aus beliebigen Wörtern bestehen und zudem beliebig tief geschachtelt werden können. 
Aufgrund dieser Punkte musste auf eine Bewertung des Systems anhand von Precision und Recall in dieser Arbeit verzichtet werden.